# Getting Started - Visual Guide

## ðŸŽ¯ What You Have

A complete **Real-time Analytics System** with:
- âœ… Kafka event streaming
- âœ… AWS Glue ETL processing
- âœ… S3 data lake storage
- âœ… < 10 seconds latency
- âœ… Automated pipeline

## ðŸ“‹ Prerequisites Checklist

- [ ] Python 3 installed
- [ ] AWS CLI installed and configured
- [ ] Kafka installed (optional for local testing)
- [ ] AWS account with appropriate permissions

## ðŸš€ Three Ways to Start

### Option 1: Full Automated Setup (Recommended)
```bash
cd '/Users/sanskargupta/Desktop/work/ARK_Infosoft/Real time Analytics System'
./setup_all.sh
```

### Option 2: Step-by-Step Setup
```bash
# 1. Install dependencies
pip3 install -r requirements.txt --break-system-packages

# 2. Configure AWS
aws configure

# 3. Create S3 buckets
aws s3 mb s3://user-events-raw
aws s3 mb s3://user-events-processed

# 4. Upload Glue scripts
aws s3 cp glue/etl_job.py s3://user-events-raw/scripts/
aws s3 cp glue/streaming_etl.py s3://user-events-raw/scripts/

# 5. Deploy infrastructure
cd config
terraform init
terraform apply
```

### Option 3: Local Testing Only
```bash
# Install Kafka
brew install kafka

# Start Kafka
./scripts/setup_kafka.sh

# Run producer (Terminal 1)
python3 kafka/producer.py

# Run consumer (Terminal 2)
python3 kafka/consumer.py
```

## ðŸ“Š Verify It's Working

### Check 1: Kafka Events
```bash
kafka-console-consumer --topic user-events \
  --bootstrap-server localhost:9092 \
  --from-beginning
```
**Expected**: Stream of JSON events

### Check 2: S3 Raw Data
```bash
aws s3 ls s3://user-events-raw/raw-events/ --recursive
```
**Expected**: JSON files organized by date

### Check 3: S3 Processed Data
```bash
aws s3 ls s3://user-events-processed/streaming/ --recursive
```
**Expected**: Parquet files partitioned by date/hour

### Check 4: Glue Catalog
```bash
aws glue get-table --database-name user_analytics_db \
  --name user_events
```
**Expected**: Table schema definition

### Check 5: Query with Athena
```sql
SELECT event_type, COUNT(*) as count
FROM user_analytics_db.user_events
WHERE date = CURRENT_DATE
GROUP BY event_type;
```
**Expected**: Event counts by type

## ðŸŽ¨ Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User App  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Events
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Kafka    â”‚ â—„â”€â”€ Producer (kafka/producer.py)
â”‚   Topic     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Stream
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Consumer   â”‚ â—„â”€â”€ Consumer (kafka/consumer.py)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Upload
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  S3 Raw     â”‚ â—„â”€â”€ JSON format
â”‚  Storage    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Process
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AWS Glue    â”‚ â—„â”€â”€ PySpark ETL (glue/streaming_etl.py)
â”‚ Streaming   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Transform
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ S3 Processedâ”‚ â—„â”€â”€ Parquet format
â”‚  Storage    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Catalog
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Glue Catalogâ”‚ â—„â”€â”€ Metadata (glue/create_catalog.py)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Query
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dashboard  â”‚ â—„â”€â”€ Athena/QuickSight
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ” Monitoring

```bash
# System health
python3 scripts/monitor.py

# Kafka consumer lag
kafka-consumer-groups --bootstrap-server localhost:9092 \
  --group analytics-consumer --describe

# Glue job status
aws glue get-job-runs --job-name user-events-etl

# CloudWatch logs
aws logs tail /aws-glue/jobs/output --follow
```

## ðŸ› Common Issues

| Issue | Solution |
|-------|----------|
| Kafka connection refused | Start Kafka: `./scripts/setup_kafka.sh` |
| AWS credentials error | Run `aws configure` |
| S3 bucket exists | Change bucket names in terraform.tf |
| Glue job fails | Check CloudWatch logs |
| Import errors | Install: `pip3 install -r requirements.txt` |

## ðŸ“ˆ Performance Tuning

- **Kafka**: Increase partitions for higher throughput
- **Glue**: Adjust DPU count for faster processing
- **S3**: Use S3 Transfer Acceleration for uploads
- **Partitioning**: Optimize based on query patterns

## ðŸŽ“ Learning Path

1. **Start**: Run local Kafka producer/consumer
2. **Deploy**: Set up AWS infrastructure
3. **Monitor**: Watch data flow through pipeline
4. **Query**: Run Athena queries on processed data
5. **Optimize**: Tune performance based on metrics

## ðŸ“š Next Steps

- [ ] Create QuickSight dashboard
- [ ] Add data quality checks
- [ ] Implement alerting
- [ ] Set up CI/CD pipeline
- [ ] Add machine learning predictions

## ðŸ’¡ Pro Tips

- Use `screen` or `tmux` for running multiple processes
- Set up CloudWatch alarms for monitoring
- Enable S3 versioning for data recovery
- Use Glue job bookmarks to avoid reprocessing
- Implement data retention policies

---

**Ready to start?** Run `./setup_all.sh` and follow the prompts!
